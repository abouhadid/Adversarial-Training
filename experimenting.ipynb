{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQqTuFHGr7g-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import * \n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "lPODGWuFsEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL ARCHITECTURE**"
      ],
      "metadata": {
        "id": "9F16r7-Is70i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_size = 1024 \n",
        "batch_size = 128\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    model_file=\"models/default_model.pth\"\n",
        "\n",
        "    def __init__(self, vgg_name):\n",
        "        super(Net, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def save(self, model_file):\n",
        "        '''Helper function, use it to save the model weights after training.'''\n",
        "        torch.save(self.state_dict(), model_file)\n",
        "\n",
        "    def load(self, model_file):\n",
        "        self.load_state_dict(torch.load(model_file, map_location=torch.device(device)))\n",
        "\n",
        "        \n",
        "    def load_for_testing(self, project_dir='./'):\n",
        "        '''This function will be called automatically before testing your\n",
        "           project, and will load the model weights from the file\n",
        "           specify in Net.model_file.\n",
        "           \n",
        "           You must not change the prototype of this function. You may\n",
        "           add extra code in its body if you feel it is necessary, but\n",
        "           beware that paths of files used in this function should be\n",
        "           refered relative to the root of your project directory.\n",
        "        '''        \n",
        "        self.load(os.path.join(project_dir, Net.model_file))\n"
      ],
      "metadata": {
        "id": "JpvswJ4MsvJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VANILLA TRAINING FUNCTION**"
      ],
      "metadata": {
        "id": "JIi_21mqtbYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(net, train_loader, pth_filename, num_epochs):\n",
        "    \n",
        "    print(\"Starting training\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    last_epoch = 0\n",
        "    mean_loss   = 0\n",
        "    mean_norm   = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for epoch in range(last_epoch, num_epochs):  \n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "          print(f\"Learning Rate : {param_group['lr']}\")\n",
        "\n",
        "        print(\"--- Training epoch {}\".format(epoch))\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            norm = None\n",
        "            loss = None\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = net(inputs)\n",
        "            loss   = F.cross_entropy(logits,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            mean_loss   += loss.detach().item()\n",
        "            \n",
        "            batch_count += 1\n",
        "            if (batch_count % 10 == 0):\n",
        "              print(f\"* Batch {i+1}/{len(train_loader)} : {mean_loss/batch_count}\")\n",
        "              mean_loss   = 0\n",
        "              batch_count = 0\n",
        "\n",
        "        net.save(pth_filename)\n",
        "        print('Model saved in {}'.format(pth_filename))\n",
        "        \n",
        "\n",
        "\n",
        "    net.save(pth_filename)\n",
        "    print('Model saved in {}'.format(pth_filename))\n",
        "    \n",
        "\n",
        "def test_natural(net, test_loader):\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for i,data in enumerate(test_loader, 0):\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(images)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "EXAKcDRgtFwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Defining main()*"
      ],
      "metadata": {
        "id": "TjIkyvf_udGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_loader(dataset, valid_size=1024, batch_size=32):\n",
        "    '''Split dataset into [train:valid] and return a DataLoader for the training part.'''\n",
        "\n",
        "    indices = list(range(len(dataset)))\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(indices[valid_size:])\n",
        "    train = torch.utils.data.DataLoader(dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train\n",
        "\n",
        "def get_validation_loader(dataset, valid_size=1024, batch_size=32):\n",
        "    '''Split dataset into [train:valid] and return a DataLoader for the validation part.'''\n",
        "\n",
        "    indices = list(range(len(dataset)))\n",
        "    valid_sampler = torch.utils.data.SubsetRandomSampler(indices[:valid_size])\n",
        "    valid = torch.utils.data.DataLoader(dataset, sampler=valid_sampler, batch_size=batch_size)\n",
        "\n",
        "    return valid\n",
        "\n",
        "def main(model_name=\"default_model\", epochs=10):\n",
        "\n",
        "    class Args:\n",
        "\n",
        "      def __init__(self, mf,ft,ne):\n",
        "        self.model_file=mf\n",
        "        self.force_train=ft\n",
        "        self.num_epochs=ne\n",
        "\n",
        "    args = Args(model_name+\".pth\",True,epochs)\n",
        "\n",
        "    \n",
        "    net = Net('VGG19')\n",
        "    net.to(device)\n",
        "\n",
        "    #### Model training (if necessary)\n",
        "    if not os.path.exists(args.model_file) or args.force_train:\n",
        "        print(\"Training model\")\n",
        "        print(args.model_file)\n",
        "\n",
        "        train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(),   \n",
        "                                               transforms.RandomCrop(size=32, padding=4), \n",
        "                                               transforms.ToTensor()]) \n",
        "        \n",
        "        cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=train_transform)\n",
        "        train_loader = get_train_loader(cifar, valid_size, batch_size=batch_size)\n",
        "        train_model(net, train_loader, args.model_file, args.num_epochs)\n",
        "        print(\"Model save to '{}'.\".format(args.model_file))\n",
        "\n",
        "    #### Model testing\n",
        "    print(\"Testing with model from '{}'. \".format(args.model_file))\n",
        "\n",
        "    # Note: You should not change the transform applied to the\n",
        "    # validation dataset since, it will be the only transform used\n",
        "    # during final testing.\n",
        "    cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=transforms.ToTensor()) \n",
        "    valid_loader = get_validation_loader(cifar, valid_size)\n",
        "\n",
        "    net.load(args.model_file)\n",
        "\n",
        "    acc = test_natural(net, valid_loader)\n",
        "    print(\"Model natural accuracy (valid): {}\".format(acc))\n",
        "\n",
        "    if args.model_file != Net.model_file:\n",
        "        print(\"Warning: '{0}' is not the default model file, \"\\\n",
        "              \"it will not be the one used for testing your project. \"\\\n",
        "              \"If this is your best model, \"\\\n",
        "              \"you should rename/link '{0}' to '{1}'.\".format(args.model_file, Net.model_file))"
      ],
      "metadata": {
        "id": "lmGRiB2uuk_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TRAINING:*"
      ],
      "metadata": {
        "id": "-M1liLPWvHeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main(\"first_model\",30)"
      ],
      "metadata": {
        "id": "PPuJK65xvGN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Testing the original model*"
      ],
      "metadata": {
        "id": "bX4nUPdtvXi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net('VGG19')\n",
        "net.to(device)\n",
        "net.load(\"first_model.pth\")\n",
        "\n",
        "cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=transforms.ToTensor()) \n",
        "test_loader = get_validation_loader(cifar, valid_size)"
      ],
      "metadata": {
        "id": "CBDQxWmdvtEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_natural(net, test_loader)"
      ],
      "metadata": {
        "id": "ydRGwPf7v5Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEFINING WHITE BOX ATTACKS**"
      ],
      "metadata": {
        "id": "pjjgMTvywH_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FGSM(model, x, y, eps=0.01, targeted=False):\n",
        "    model.eval()            \n",
        "    x.requires_grad_(True)  \n",
        "\n",
        "    logits = model(torch.clamp(x,0,1))\n",
        "    loss   = F.cross_entropy(logits, y)\n",
        "    loss.backward()\n",
        "\n",
        "    if not targeted:    x_adv  = x.detach() + eps * torch.sign(x.grad)  \n",
        "    else:               x_adv  = x.detach() - eps * torch.sign(x.grad)  \n",
        "\n",
        "    x = x.detach()\n",
        "    model.zero_grad()\n",
        "    return x_adv.detach()"
      ],
      "metadata": {
        "id": "yKQ_bXaawAMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#projecting points onto the lp-norm if outside\n",
        "def project(x, eps, p=2):\n",
        "    if p==\"inf\":\n",
        "        x = torch.clamp(x,-eps,eps)\n",
        "    else:\n",
        "        norms = torch.norm(x.view(x.size(0),-1),dim=1,p=p)\n",
        "        norms[norms==0] = 1                          \n",
        "        mask  = (norms > eps)                        \n",
        "        x[mask] /= norms[mask].view(-1,1,1,1)        \n",
        "        x[mask] *= eps                               \n",
        "    return x\n",
        "\n",
        "\n",
        "def PGD(model, x, y, eps=0.1, stepsize=0.04, iterations=8, p=2, targeted=False):\n",
        "    model.eval()  \n",
        "\n",
        "    \n",
        "    if p==\"inf\":\n",
        "        delta = (torch.rand(x.size()) * 2 - 1) * eps   \n",
        "    else:\n",
        "        delta = torch.randn_like(x)                                             \n",
        "        norms = torch.norm(delta.view(delta.size(0),-1), dim=1, p=p)\n",
        "        norms[norms==0] = 1                                                     \n",
        "        delta /= norms.view(-1,1,1,1)                                           \n",
        "        delta *= torch.rand((delta.size(0),1,1,1)).to(device) * eps                       \n",
        "\n",
        "    delta = delta.to(device)\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        x_adv = x + delta\n",
        "        x_adv.requires_grad_(True)\n",
        "        logits = model(torch.clamp(x_adv,0,1))\n",
        "        loss   = F.cross_entropy(logits,y)\n",
        "        loss.backward()\n",
        "\n",
        "        \n",
        "        if p==\"inf\":\n",
        "            gradient = torch.sign(x_adv.grad) * stepsize\n",
        "        else:\n",
        "            gradient  = x_adv.grad                                                \n",
        "            norms     = torch.norm(gradient.view(gradient.size(0),-1),dim=1,p=p)\n",
        "            norms[norms==0] = 1                                                   \n",
        "            gradient /= norms.view(-1,1,1,1)                                      \n",
        "            gradient *= stepsize                                                  \n",
        "\n",
        "        if not targeted :   x_adv = x_adv.detach() + gradient \n",
        "        else:               x_adv = x_adv.detach() - gradient\n",
        "\n",
        "        \n",
        "        delta = (x_adv - x).detach()\n",
        "        delta = project(delta, eps, p)\n",
        "\n",
        "    x = x.detach()\n",
        "    model.zero_grad()\n",
        "    return torch.clamp(x + delta,0,1)\n"
      ],
      "metadata": {
        "id": "Z5U8Y1hVwUEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*White box attacks impact on vanilla model*"
      ],
      "metadata": {
        "id": "UO9uoZTgxYSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing \n",
        "def test_adv(net, test_loader, attack):\n",
        "    \n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    for i,data in enumerate(test_loader, 0):\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        if attack == 'FGSM': \n",
        "          images_adv = FGSM(net, images, labels)\n",
        "        elif attack == 'PGD-l2': \n",
        "          images_adv = PGD(net, images, labels)\n",
        "        else:\n",
        "          images_adv = PGD(net, images, labels, eps=0.01, stepsize=0.004, p='inf')\n",
        "\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images_adv)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "PyJsuQIFxO13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model test accuracy on FGSM-attacked set is\", test_adv(net, test_loader,\"FGSM\"))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on PGD l2-attacked set is\", test_adv(net, test_loader,\"PGD-l2\"))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on PGD linf-attacked set is\", test_adv(net, test_loader, \"PGD-linf\"))"
      ],
      "metadata": {
        "id": "-AWTVXwM0CfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ADVERSARIAL TRAINING**"
      ],
      "metadata": {
        "id": "-J_AEabk1DgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tried adversarial training with multiple combinaison of attacks and ratios introduced on the train loader. The best best set was training on 50% of original images and 50% on images attacked with PGD-linf"
      ],
      "metadata": {
        "id": "1zrPxjXO2ML0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def adv_training(net, train_loader, pth_filename, num_epochs):\n",
        "    \n",
        "    print(\"Starting training\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    last_epoch = 0\n",
        "    mean_loss   = 0\n",
        "    mean_norm   = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for epoch in range(last_epoch, num_epochs):  \n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "          print(f\"Learning Rate : {param_group['lr']}\")\n",
        "\n",
        "        print(\"--- Training epoch {}\".format(epoch))\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            adv_inputs=PGD(net,inputs,labels, p='inf')\n",
        "            concatenated_images = torch.cat((adv_inputs, inputs), 0)\n",
        "            concatenated_targets = torch.cat((labels,labels), 0)\n",
        "            norm = None\n",
        "            loss = None\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = net(concatenated_images)\n",
        "            loss   = F.cross_entropy(logits,concatenated_targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            mean_loss   += loss.detach().item()\n",
        "            \n",
        "            batch_count += 1\n",
        "            if (batch_count % 10 == 0):\n",
        "              print(f\"* Batch {i+1}/{len(train_loader)} : {mean_loss/batch_count}\")\n",
        "              mean_loss   = 0\n",
        "              batch_count = 0\n",
        "\n",
        "        net.save(pth_filename)\n",
        "        print('Model saved in {}'.format(pth_filename))\n",
        "        \n",
        "\n",
        "\n",
        "    net.save(pth_filename)\n",
        "    print('Model saved in {}'.format(pth_filename))"
      ],
      "metadata": {
        "id": "bk-TbW020vE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main2(model_name=\"default_model\", epochs=10):\n",
        "\n",
        "    class Args:\n",
        "\n",
        "      def __init__(self, mf,ft,ne):\n",
        "        self.model_file=mf\n",
        "        self.force_train=ft\n",
        "        self.num_epochs=ne\n",
        "\n",
        "    args = Args(model_name+\".pth\",True,epochs)\n",
        "\n",
        "    \n",
        "    net = Net('VGG19')\n",
        "    net.to(device)\n",
        "\n",
        "    #### Model training (if necessary)\n",
        "    if not os.path.exists(args.model_file) or args.force_train:\n",
        "        print(\"Training model\")\n",
        "        print(args.model_file)\n",
        "\n",
        "        train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(),   \n",
        "                                               transforms.RandomCrop(size=32, padding=4), \n",
        "                                               transforms.ToTensor()]) \n",
        "        \n",
        "        cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=train_transform)\n",
        "        train_loader = get_train_loader(cifar, valid_size, batch_size=batch_size)\n",
        "        adv_training(net, train_loader, args.model_file, args.num_epochs)\n",
        "        print(\"Model save to '{}'.\".format(args.model_file))\n",
        "\n",
        "    #### Model testing\n",
        "    print(\"Testing with model from '{}'. \".format(args.model_file))\n",
        "\n",
        "    # Note: You should not change the transform applied to the\n",
        "    # validation dataset since, it will be the only transform used\n",
        "    # during final testing.\n",
        "    cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=transforms.ToTensor()) \n",
        "    valid_loader = get_validation_loader(cifar, valid_size)\n",
        "\n",
        "    net.load(args.model_file)\n",
        "\n",
        "    acc = test_natural(net, valid_loader)\n",
        "    print(\"Model natural accuracy (valid): {}\".format(acc))\n",
        "\n",
        "    if args.model_file != Net.model_file:\n",
        "        print(\"Warning: '{0}' is not the default model file, \"\\\n",
        "              \"it will not be the one used for testing your project. \"\\\n",
        "              \"If this is your best model, \"\\\n",
        "              \"you should rename/link '{0}' to '{1}'.\".format(args.model_file, Net.model_file))"
      ],
      "metadata": {
        "id": "IjMhhmkI0pNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main2(\"robust_model\",30)"
      ],
      "metadata": {
        "id": "lehDaSQq3ASk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the adversarially trained model"
      ],
      "metadata": {
        "id": "ySDsd5zE3KYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_adv = Net('VGG19')\n",
        "net_adv.to(device)\n",
        "net_adv.load(\"robust_model.pth\")"
      ],
      "metadata": {
        "id": "2FPZuQNb3Jf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model test accuracy on original set is:\", test_natural(net_adv, test_loader))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on FGSM-attacked set is:\", test_adv(net_adv, test_loader,\"FGSM\"))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on PGD l2-attacked set is:\", test_adv(net_adv, test_loader,\"PGD-l2\"))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on PGD linf-attacked set is:\", test_adv(net_adv, test_loader, \"PGD-linf\"))"
      ],
      "metadata": {
        "id": "HEOTRVDg3yFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Epsilon effect on model accuracy"
      ],
      "metadata": {
        "id": "Q0ytQUOO4hYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph(model, epsilons, attack):\n",
        "  accuracies = []\n",
        "  for eps in epsilons:\n",
        "    if attack == \"FGSM\":      attack_fn = (lambda net,input,label : FGSM(net,input,label,eps,False))\n",
        "    elif attack == \"PGD-l2\":  attack_fn = (lambda net,input,label : PGD(net,input,label,eps,eps/10,10,2,False))\n",
        "    else:                     attack_fn = (lambda net,input,label : PGD(net,input,label,eps,eps/10,10,\"inf\",False))\n",
        "    acc = test_adv(model, test_loader, attack_fn)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "  return accuracies"
      ],
      "metadata": {
        "id": "d4LKHOn14fon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*for FGSM attack*"
      ],
      "metadata": {
        "id": "77sp8vP95QCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilons = [i*0.01 for i in range(21)]\n",
        "accuracies_normal = get_graph(net, epsilons, \"FGSM\")\n",
        "accuracies_adv    = get_graph(net_adv, epsilons, \"FGSM\")"
      ],
      "metadata": {
        "id": "AVqfWG-D5He7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epsilons, accuracies_normal, color=\"b\", label=\"Vanilla Training\")\n",
        "plt.plot(epsilons, accuracies_adv, color=\"r\", label=\"Adversarial Training\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OJ0xygBR5VL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*for PGD-l2 attack*"
      ],
      "metadata": {
        "id": "wXNMaxq45Yr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilons = [i*0.1 for i in range(21)]\n",
        "accuracies_normal = get_graph(net, epsilons, \"PGD-l2\")\n",
        "accuracies_adv    = get_graph(net_adv, epsilons, \"PGD-l2\")"
      ],
      "metadata": {
        "id": "egpYu1aG5cUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epsilons, accuracies_normal, color=\"b\", label=\"Vanilla Training\")\n",
        "plt.plot(epsilons, accuracies_adv, color=\"r\", label=\"Adversarial Training\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kBtM-kWC5nYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*for PGD-linf attack*"
      ],
      "metadata": {
        "id": "ht9Ide9o5q1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilons = [i*0.01 for i in range(21)]\n",
        "accuracies_normal = get_graph(net, epsilons, \"PGD-linf\")\n",
        "accuracies_adv    = get_graph(net_adv, epsilons, \"PGD-linf\")"
      ],
      "metadata": {
        "id": "Pu0QZ4Kf5tYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epsilons, accuracies_normal, color=\"b\", label=\"Vanilla Training\")\n",
        "plt.plot(epsilons, accuracies_adv, color=\"r\", label=\"Adversarial Training\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7zEvnbMw5wWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Black Box Attack 1: NES**"
      ],
      "metadata": {
        "id": "S42nJfcqsjfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fonction NES pour approx du gradient\n",
        "def NES(model, image, label, sigma, n):\n",
        "    # Returns estimate of the gradient using NES approach\n",
        "    img_rows, img_cols, channels = 32, 32, 3\n",
        "    N = img_rows * img_cols * channels\n",
        "    x = torch.Tensor(image.reshape(N)).to(device)\n",
        "    gradient = torch.Tensor(np.zeros((1, channels,img_rows, img_cols ),dtype=np.float32)).to(device)\n",
        "    cce = nn.CrossEntropyLoss()\n",
        "    if label.shape!=(1,10):\n",
        "      label=torch.nn.functional.one_hot(label, num_classes=10).reshape((1,10)).to(device).float()\n",
        "    for i in range(n):\n",
        "        \n",
        "        delta_i = np.random.normal(0,1,N)\n",
        "        delta_i=delta_i.astype(np.float32)\n",
        "        delta_i=torch.Tensor(delta_i).to(device)\n",
        "        theta_pos = x + sigma * delta_i\n",
        "        theta_neg = x - sigma * delta_i\n",
        "        theta_pos = torch.Tensor(theta_pos.reshape((1, channels,img_rows, img_cols))).to(device)\n",
        "        theta_neg = torch.Tensor(theta_neg.reshape((1,channels, img_rows, img_cols))).to(device)\n",
        "        prediction_pos = model(theta_pos).float()\n",
        "        prediction_neg = model(theta_neg)\n",
        "\n",
        "        loss_pos = - cce(label, prediction_pos)\n",
        "        loss_neg = - cce(label, prediction_neg)\n",
        "\n",
        "        gradient += loss_pos.item() * delta_i.reshape((1, channels,img_rows, img_cols ))\n",
        "        gradient -= loss_neg.item() * delta_i.reshape((1, channels,img_rows, img_cols))\n",
        "    \n",
        "    gradient /= 2*n*sigma\n",
        "  \n",
        "    return gradient"
      ],
      "metadata": {
        "id": "YSJnfG0Usr2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def projection(sample, norm, epsilon):\n",
        "\n",
        "    # Clipping perturbation sample to norm ball\n",
        "    if norm not in [np.inf, 1, 2]:\n",
        "        raise ValueError('ord must be np.inf, 1, or 2.')\n",
        "\n",
        "    if norm == np.inf:\n",
        "        sample = np.clip(sample.cpu(), -epsilon, epsilon)\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "qLnmwagPsw8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PGDBlackBox(model,niter, image, label, eta, epsilon, sigma, n): \n",
        "    cost = np.zeros(niter)\n",
        "    x0 = image\n",
        "    x_adv = image\n",
        "    loss=nn.CrossEntropyLoss()\n",
        "    img_rows, img_cols, channels = 32, 32, 3\n",
        "    \n",
        "    for t in range(niter): \n",
        "\n",
        "        cost[t] = loss( model(x_adv),label)\n",
        "        \n",
        "        x_adv=x_adv.type(torch.FloatTensor)\n",
        "        GradEstim = NES(model, x_adv, label, sigma, n)\n",
        "        \n",
        "        signed_grad = GradEstim.sign().to(device)\n",
        "        \n",
        "        x_adv=x_adv.to(device)\n",
        "        x_adv = x_adv - eta*signed_grad\n",
        "        \n",
        "        perturb = x_adv - x0\n",
        "        \n",
        "        perturb = torch.Tensor(projection(perturb, np.inf, epsilon)).to(device)\n",
        "        x_adv = x0 + perturb\n",
        "        \n",
        "\n",
        "        x_adv = np.clip(x_adv.cpu(), 0, 255) # ensure it's in the good range\n",
        "        x_adv=x_adv.to(device)\n",
        "    print(\"PGD Black Box done...\")\n",
        "    return (x_adv, cost)"
      ],
      "metadata": {
        "id": "C1KomeREszra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "niter = 30\n",
        "eta = 0.01\n",
        "epsilon = 0.5\n",
        "sigma = 0.01\n",
        "n_ = 50\n",
        "\n",
        "#number of examples\n",
        "n_examples=10\n",
        "\n",
        "#calculate the accuracy\n",
        "total=0\n",
        "adv_correct=0\n",
        "normal_correct=0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i,data in enumerate(test_loader, 0):\n",
        "    inputs, targets = data[0].to(device), data[1].to(device)\n",
        "    for i in range(inputs.shape[0]):\n",
        "      image=inputs[i].reshape((1,3,32,32))\n",
        "      label=targets[i].reshape((1))\n",
        "      x_PGD, cost_PGD = PGDBlackBox(net,niter, image, label, eta, epsilon, sigma, n_)\n",
        "      _,adv_prediction=net(x_PGD).max(1)\n",
        "      _,prediction=net(image).max(1)\n",
        "      total+=1\n",
        "      if adv_prediction.eq(label):\n",
        "        adv_correct+=1\n",
        "      if prediction.eq(label):\n",
        "        normal_correct+=1\n",
        "      print('\\nthe number of generated images :'+str(total))\n",
        "\n",
        "      if total>=n_examples:\n",
        "        break\n",
        "    break\n",
        "\n",
        "print('Natural accuracy :'+str(normal_correct*100/total))\n",
        "print('Adverarial accuracy :'+str(adv_correct*100/total))\n",
        "\n",
        "#if it does not give good results please check your original model  "
      ],
      "metadata": {
        "id": "0rSUme5fs5j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Black Box Attack 2: INJECTING RANDOM NOISE**"
      ],
      "metadata": {
        "id": "h5mngt3stITH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing \n",
        "def test_noised(net):\n",
        "    \n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    valid_loader = get_noised_test_loader(noise=0.1)\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    for i,data in enumerate(valid_loader, 0):\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "o-aHGHbatHox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Testing injecting noise effect on the models trained previously:*\n"
      ],
      "metadata": {
        "id": "ix3EcFm0uKtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vanilla model test accuracy on noised set is:\", test_noised(net))\n",
        "print('\\n')\n",
        "print(\"Adversarial model accuracy on noised set is:\", test_noised(net_adv))\n"
      ],
      "metadata": {
        "id": "gjWKJ7DquT-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New Defense Approach: Randomized Network**"
      ],
      "metadata": {
        "id": "FiC5g0bqAar0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is achieved by injecting gaussian noise to the train loader.\n",
        "We used this method simultaneously with adversarial training: Regularized Adversarial Training (**RAT**)"
      ],
      "metadata": {
        "id": "2mz7Et_tArWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_size = 1024\n",
        "batch_size = 32\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={}, std={})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "\n",
        "def get_noised_train_loader(noise, valid_size=valid_size, batch_size=batch_size):\n",
        "    cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=transforms.Compose(\n",
        "        [transforms.RandomHorizontalFlip(), transforms.RandomCrop(size=32, padding=4), transforms.ToTensor(), AddGaussianNoise(0., noise)]))\n",
        "    train_loader = get_train_loader(cifar, valid_size, batch_size=batch_size)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def get_noised_test_loader(noise, valid_size=valid_size):\n",
        "    cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=transforms.Compose(\n",
        "        [transforms.ToTensor(), AddGaussianNoise(0., noise)]))\n",
        "    valid_loader = get_validation_loader(cifar, valid_size, 1)  \n",
        "    return valid_loader\n",
        "\n",
        "\n",
        "\n",
        "def training(net, train_loader, num_epochs):\n",
        "    '''Basic training function'''\n",
        "    print(\"Starting training\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=1e-3,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    train_loss = []\n",
        "    for epoch in range(num_epochs): \n",
        "\n",
        "        loss_per_epoch = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            \n",
        "          \n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "            \n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss_per_epoch += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss = loss_per_epoch / len(train_loader)\n",
        "        train_loss.append(epoch_loss)\n",
        "        '''print(f'epoch : {epoch} --- loss : {epoch_loss} ')'''\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "def train_network(noise, epochs=20):\n",
        "    \n",
        "    net = Net('VGG19')\n",
        "    net.to(device)\n",
        "    \n",
        "    train_loader = get_noised_train_loader(noise=noise)\n",
        "    \n",
        "    loss = training(net, train_loader, epochs)\n",
        "    # VALIDATION\n",
        "    acc, acc_noised = natural_acc(net, noise)  \n",
        "    print(\"Model trained with gaussian noise of {}, final loss of {}.\\n \"\n",
        "          \"Natural accuracy on classic test data is : {}, natural accuracy on noised test data is {}\".format(noise,\n",
        "                                                                                                             loss, acc,\n",
        "                                                                                                             acc_noised))\n",
        "    return [acc, acc_noised]\n",
        "    \n",
        "def natural_acc(net, noise):\n",
        "    valid_noised_loader = get_noised_test_loader(noise=noise)\n",
        "    valid_loader = get_noised_test_loader(noise=0)\n",
        "\n",
        "    acc = test_natural(net, valid_loader)\n",
        "    acc_noised = test_natural(net, valid_noised_loader)\n",
        "    return acc, acc_noised\n",
        "\n",
        "def train_multiple(noises, epochs=10):\n",
        "    a=[]\n",
        "    b=[]\n",
        "    \n",
        "    for n in noises:\n",
        "      N=train_network(n, epochs)\n",
        "\n",
        "      a=a.append(train_network(n, epochs)[0])\n",
        "      b=b.append(train_network(n, epochs)[1])\n",
        "      \n",
        "    \n",
        "    return [a,b]\n",
        "      "
      ],
      "metadata": {
        "id": "PkECAEk4AoJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noiz=[0, 0.01, 0.02, 0.03, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.5]"
      ],
      "metadata": {
        "id": "vKGZB6ObEAbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=train_multiple(noiz, epochs=10)\n",
        "X"
      ],
      "metadata": {
        "id": "sdLINqm9EHNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#num. of epochs\n",
        "ep=[i for i in range(1,11)]\n",
        "#training losses\n",
        "noise_0=[1.2888056789248072, 0.7869450969586257, 0.5858477791935537, 0.4466561506837509, 0.3529048065084947, 0.2696410481721299, 0.21334297112709338, 0.16467761626901442, 0.13839442336233485, 0.10238422836547512]\n",
        "noise_001=[1.3108496756276455, 0.8074855775828458, 0.6020222639702723, 0.46687756462727087, 0.3660965919407032, 0.2803657855108565, 0.2205072932528085, 0.17605155517647053, 0.14132746913641264, 0.11310747206307811]\n",
        "noise_002=[1.3144987444329463, 0.8211092451364678, 0.6202640661143696, 0.48477185698582564, 0.38047535907766694, 0.30434330958680805, 0.23690595432439518, 0.19122005370460757, 0.1533006953226389, 0.12615584611289257]\n",
        "noise_003=[1.3394614815089065, 0.8345390617224533, 0.6299919661044452, 0.49970256258349727, 0.40080077866862446, 0.32065984316700663, 0.2600182276255628, 0.21124502746990542, 0.17196085921304385, 0.1360492542855062]\n",
        "noise_005=[1.3632234056193715, 0.8770833308987956, 0.6791906547577532, 0.5516207414337967, 0.45026004612251175, 0.3704747513817425, 0.30685885210663255, 0.2488678347801146, 0.214132045612839, 0.17370501224676685]\n",
        "noise_006=[1.3568309193962467, 0.8820886028544876, 0.6923485718214925, 0.5666812164362235, 0.4641273476275878, 0.3866256887761111, 0.32287008736761935, 0.26942535708302534, 0.22881253791947445, 0.191705191451971]\n",
        "noise_007=[1.3793536864566616, 0.9039732478242696, 0.7040254888796946, 0.5778023909794435, 0.48551854992967897, 0.40402435751268084, 0.3382490313910605, 0.2875962384132721, 0.24162909669033905, 0.2047734524334454]\n",
        "noise_008=[1.3974817660336087, 0.9324690723909731, 0.7387780636490598, 0.6066868591744631, 0.5179838278981146, 0.44347720327017587, 0.3731494717139344, 0.31912902047204317, 0.26516474887684893, 0.2277376352645734]\n",
        "noise_009=[1.3987721760704344, 0.9462217088036876, 0.758692577919253, 0.6307238738845174, 0.5354199004593743, 0.46307728849813723, 0.3962152064342969, 0.33665184197055603, 0.2893354245561895, 0.2530678095822822]\n",
        "noise_01=[1.4119416697596818, 0.962810063307617, 0.7738572548293818, 0.6498156562885531, 0.5514199158418888, 0.48181529172314763, 0.4093757550372632, 0.3485679674156346, 0.3055674814777893, 0.2662686526709663]\n",
        "noise_02=[1.5014243485256398, 1.124355860623871, 0.9570515760793007, 0.8484151301665839, 0.7688898360277608, 0.6969107354119429, 0.6326694576900664, 0.5781421898763839, 0.5320383878333517, 0.4826074579400926]\n",
        "noise_03=[1.5731349808184014, 1.2407874354422288, 1.1008662357975034, 1.0111463333484318, 0.9263936278673655, 0.8700584016501864, 0.8125378827422682, 0.7716610119243138, 0.7230966089988051, 0.6761001867359562]\n",
        "noise_05=[1.6954003872239145, 1.4403352342268596, 1.3289815649575147, 1.259240535059293, 1.1964331646124418, 1.1480693524371246, 1.10686845560622, 1.075230516459255, 1.0390504901507258, 1.0049954271643555]"
      ],
      "metadata": {
        "id": "c2lS6xufESP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(10, 6), dpi=80)\n",
        "plt.plot(ep, noise_0, label=\"training without noise\")\n",
        "plt.plot(ep, noise_001, label=\"injecting noise=0.01\")\n",
        "plt.plot(ep, noise_002, label=\"injecting noise=0.02\")\n",
        "plt.plot(ep, noise_003, label=\"injecting noise=0.03\")\n",
        "plt.plot(ep, noise_005, label=\"injecting noise=0.05\")\n",
        "plt.plot(ep, noise_006, label=\"injecting noise=0.06\")\n",
        "plt.plot(ep, noise_007,  label=\"injecting noise=0.07\")\n",
        "plt.plot(ep, noise_008, label=\"injecting noise=0.08\")\n",
        "plt.plot(ep, noise_009, label=\"injecting noise=0.09\")\n",
        "plt.plot(ep, noise_01, label=\"injecting noise=0.1\")\n",
        "plt.plot(ep, noise_02, label=\"injecting noise=0.2\")\n",
        "plt.plot(ep, noise_03, label=\"injecting noise=0.3\")\n",
        "plt.plot(ep, noise_05, label=\"injecting noise=0.5\")\n",
        "\n",
        "#plt.plot(epsilons, accuracies_grad, color=\"g\", label=\"Gradient Minimization\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
        "          ncol=2, fancybox=True, shadow=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9EAlS7HW5p8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy=X[0]\n",
        "noised_test_accuracy=X[1]"
      ],
      "metadata": {
        "id": "8sg3MEq7AWe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(8, 4), dpi=80)\n",
        "plt.plot(noiz, noised_test_accuracy, label=\"Accuracy on noised test set\")\n",
        "plt.plot(noiz, test_accuracy, label=\"Accuracy on original test set\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Injected noise\")\n",
        "plt.legend(loc='best',\n",
        "          ncol=1, fancybox=True, shadow=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2WRG5r7dJVZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Injecting noise with sigma=0.06 achieves the best accuracy on both test and noised-test sets.\n",
        "Now we train a model adversarially by introducing a noise in the training loader and we test the module robustness."
      ],
      "metadata": {
        "id": "oCJrEG5SJcXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main3(model_name=\"default_model\", epochs=10):\n",
        "\n",
        "    class Args:\n",
        "\n",
        "      def __init__(self, mf,ft,ne):\n",
        "        self.model_file=mf\n",
        "        self.force_train=ft\n",
        "        self.num_epochs=ne\n",
        "\n",
        "    args = Args(model_name+\".pth\",True,epochs)\n",
        "\n",
        "    \n",
        "    net = Net('VGG19')\n",
        "    net.to(device)\n",
        "\n",
        "    #### Model training (if necessary)\n",
        "    if not os.path.exists(args.model_file) or args.force_train:\n",
        "        print(\"Training model\")\n",
        "        print(args.model_file)\n",
        "\n",
        "         \n",
        "        \n",
        "        train_loader_noised = get_noised_train_loader(noise=0.06\n",
        "                                                      )\n",
        "        adv_training(net, train_loader_noised, args.model_file, args.num_epochs)\n",
        "        print(\"Model save to '{}'.\".format(args.model_file))\n",
        "\n",
        "    #### Model testing\n",
        "    print(\"Testing with model from '{}'. \".format(args.model_file))\n",
        "\n",
        "    # Note: You should not change the transform applied to the\n",
        "    # validation dataset since, it will be the only transform used\n",
        "    # during final testing.\n",
        "    cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=transforms.ToTensor()) \n",
        "    valid_loader = get_validation_loader(cifar, valid_size)\n",
        "\n",
        "    net.load(args.model_file)\n",
        "\n",
        "    acc = test_natural(net, valid_loader)\n",
        "    print(\"Model natural accuracy (valid): {}\".format(acc))\n",
        "\n",
        "    if args.model_file != Net.model_file:\n",
        "        print(\"Warning: '{0}' is not the default model file, \"\\\n",
        "              \"it will not be the one used for testing your project. \"\\\n",
        "              \"If this is your best model, \"\\\n",
        "              \"you should rename/link '{0}' to '{1}'.\".format(args.model_file, Net.model_file))"
      ],
      "metadata": {
        "id": "1LKS9TMYKWvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main3(\"RAT_model\",30)"
      ],
      "metadata": {
        "id": "dG1kk-hxMl8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Testing the model trained by injecting noise*"
      ],
      "metadata": {
        "id": "-vGcrtPdM0p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_rat = Net('VGG19')\n",
        "net_rat.to(device)\n",
        "net_rat.load(\"RAT_model.pth\")"
      ],
      "metadata": {
        "id": "fGZk1GREMu9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model test accuracy on original set is:\", test_natural(net_rat, test_loader))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on FGSM-attacked set is:\", test_adv(net_rat, test_loader,\"FGSM\"))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on PGD l2-attacked set is:\", test_adv(net_rat, test_loader,\"PGD-l2\"))\n",
        "print('\\n')\n",
        "print(\"Model test accuracy on PGD linf-attacked set is:\", test_adv(net_rat, test_loader, \"PGD-linf\"))"
      ],
      "metadata": {
        "id": "i1TH-2vQNJP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING MODEL ARCHITECTURE IMPACT ON MODEL ROBUSTNESS**"
      ],
      "metadata": {
        "id": "ADDUbDViNN_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we trained 4 variants of VGG model (VGG11, VGG13, VGG16 and VGG19). We used adversarial training under the same conditions (same optimizer settings, number of epochs..)"
      ],
      "metadata": {
        "id": "5Gd1ul_SNVVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This following dictionary is used to build the models:"
      ],
      "metadata": {
        "id": "1zjTr7TdN6y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n"
      ],
      "metadata": {
        "id": "nZcDbmBTOFDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and savind the models:"
      ],
      "metadata": {
        "id": "hsKTrScUOUhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main4(model_name=\"model\", epochs=10):\n",
        "\n",
        "    '''\n",
        "    #### Parse command line arguments \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model-file\", default=Net.model_file,\n",
        "                        help=\"Name of the file used to load or to sore the model weights.\"\\\n",
        "                        \"If the file exists, the weights will be load from it.\"\\\n",
        "                        \"If the file doesn't exists, or if --force-train is set, training will be performed, \"\\\n",
        "                        \"and the model weights will be stored in this file.\"\\\n",
        "                        \"Warning: \"+Net.model_file+\" will be used for testing (see load_for_testing()).\")\n",
        "    parser.add_argument('-f', '--force-train', action=\"store_true\",\n",
        "                        help=\"Force training even if model file already exists\"\\\n",
        "                             \"Warning: previous model file will be erased!).\")\n",
        "    parser.add_argument('-e', '--num-epochs', type=int, default=10,\n",
        "                        help=\"Set the number of epochs during training\")\n",
        "    args = parser.parse_args()\n",
        "    '''\n",
        "\n",
        "    class Args:\n",
        "\n",
        "      def __init__(self, mf,ft,ne):\n",
        "        self.model_file=mf\n",
        "        self.force_train=ft\n",
        "        self.num_epochs=ne\n",
        "    for key in cfg:\n",
        "\n",
        "      args = Args(key+\".pth\",True,epochs)\n",
        "\n",
        "    #### Create model and move it to whatever device is available (gpu/cpu)\n",
        "      net = Net(key)\n",
        "      net.to(device)\n",
        "\n",
        "    #### Model training (if necessary)\n",
        "      if not os.path.exists(args.model_file) or args.force_train:\n",
        "        print(\"Training model\")\n",
        "        print(args.model_file)\n",
        "\n",
        "        train_transform = transforms.Compose([ transforms.RandomHorizontalFlip(),   \n",
        "                                               transforms.RandomCrop(size=32, padding=4), \n",
        "                                               transforms.ToTensor()]) \n",
        "        \n",
        "        cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=train_transform)\n",
        "        train_loader = get_train_loader(cifar, valid_size, batch_size=batch_size)\n",
        "        adv_training(net, train_loader, args.model_file, args.num_epochs)\n",
        "        print(\"Model save to '{}'.\".format(args.model_file))\n",
        "\n",
        "    #### Model testing\n",
        "      print(\"Testing with model from '{}'. \".format(args.model_file))\n",
        "\n",
        "    # Note: You should not change the transform applied to the\n",
        "    # validation dataset since, it will be the only transform used\n",
        "    # during final testing.\n",
        "      cifar = torchvision.datasets.CIFAR10('./data/', download=True, transform=transforms.ToTensor()) \n",
        "      valid_loader = get_validation_loader(cifar, valid_size)\n",
        "\n",
        "      net.load(args.model_file)\n",
        "\n",
        "      acc = test_natural(net, valid_loader)\n",
        "      print(\"Model natural accuracy (valid): {}\".format(acc))\n",
        "\n",
        "      if args.model_file != Net.model_file:\n",
        "        print(\"Warning: '{0}' is not the default model file, \"\\\n",
        "              \"it will not be the one used for testing your project. \"\\\n",
        "              \"If this is your best model, \"\\\n",
        "              \"you should rename/link '{0}' to '{1}'.\".format(args.model_file, Net.model_file))"
      ],
      "metadata": {
        "id": "37oGYzPgOMjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main4(epochs=10)"
      ],
      "metadata": {
        "id": "jitxRdA_OfBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Testing the models*"
      ],
      "metadata": {
        "id": "Fa0Dlo0FPdXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o=[]\n",
        "f=[]\n",
        "p1=[]\n",
        "p2=[]\n",
        "for key in cfg:\n",
        "  print('for the'+key+\"-model:\")\n",
        "\n",
        "  path=key+\".pth\"\n",
        "  net = Net(key)\n",
        "  net.to(device)\n",
        "  net.load(path)\n",
        "\n",
        "  print(\"Model test accuracy on original set is:\", test_natural(net, test_loader))\n",
        "  o.append(test_natural(net, test_loader))\n",
        "  print(\"Model test accuracy on FGSM-attacked set is:\", test_adv(net, test_loader,\"FGSM\"))\n",
        "  f.append(test_adv(net, test_loader,\"FGSM\"))\n",
        "  print(\"Model test accuracy on PGD l2-attacked set is:\", test_adv(net, test_loader,\"PGD-l2\"))\n",
        "  p1.append(test_adv(net, test_loader,\"PGD-l2\"))\n",
        "  print(\"Model test accuracy on PGD linf-attacked set is:\", test_adv(net, test_loader, \"PGD-linf\"))\n",
        "  p2.append(test_adv(net, test_loader, \"PGD-linf\"))\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "of6ktBYFPIHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Plotting the results*"
      ],
      "metadata": {
        "id": "LSPAMMkHTWjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M=['VGG11','VGG13','VGG16','VGG19']\n",
        "\n",
        "figure(figsize=(12, 6), dpi=80)\n",
        "plt.plot(M, o, label=\"Accuracy on original test set\")\n",
        "plt.plot(M, f, label=\"Accuracy on FGSM-attacked test set\")\n",
        "plt.plot(M,p1, label=\"Accuracy on PGD l2-attacked test set\")\n",
        "plt.plot(M, p2, label=\"Accuracy on PGD ling-attacked test set\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Models\")\n",
        "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
        "          ncol=2, fancybox=True, shadow=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EPjeTtW-Q8o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rnht2xf0RwzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fWREr4WhTVPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}